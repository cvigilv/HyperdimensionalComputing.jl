var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API","title":"API Reference","text":"This page contains the complete API reference for HyperdimensionalComputing.jl.","category":"section"},{"location":"api/#Index","page":"API","title":"Index","text":"Pages = [\"api.md\"]","category":"section"},{"location":"api/#Functions","page":"API","title":"Functions","text":"","category":"section"},{"location":"api/#Types","page":"API","title":"Types","text":"","category":"section"},{"location":"api/#Constants","page":"API","title":"Constants","text":"","category":"section"},{"location":"api/#Macros","page":"API","title":"Macros","text":"","category":"section"},{"location":"api/#Base.isapprox-Union{Tuple{T}, Tuple{T, T}} where T<:AbstractHV","page":"API","title":"Base.isapprox","text":"Base.isapprox(u::AbstractHV, v::AbstractHV, atol=length(u)/100, ptol=0.01)\n\nMeasurures when two hypervectors are similar (have more elements in common than expected by chance) using the Hamming distance. Uses a bootstrap to construct a null distribution.\n\nOne can specify either:\n\nptol=1e-10 threshold for seeing that many matches due to chance\nN_bootstap=200 number of samples for bootstrapping\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.isapprox-Union{Tuple{T}, Tuple{T, T}} where T<:Union{BinaryHV, BipolarHV}","page":"API","title":"Base.isapprox","text":"Base.isapprox(u::AbstractHV, v::AbstractHV, atol=length(u)/100, ptol=0.01)\n\nMeasurures when two hypervectors are similar (have more elements in common than expected by chance).\n\nOne can specify either:\n\natol=N/100 number of matches more than due to chance needed for being assumed similar\nptol=0.01 threshold for seeing that many matches due to chance\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.bindsequence-Tuple{AbstractVector{<:AbstractHV}}","page":"API","title":"HyperdimensionalComputing.bindsequence","text":"bindsequence(vs::AbstractVector{<:AbstractHV})\n\nBinding-based sequence. The first value is not permuted, the last value is permuted n-1 times.\n\nArguments\n\nvs::AbstractVector{<:AbstractHV}: Hypervector sequence\n\nExamples\n\njulia> vs = [BinaryHV(10) for _ in 1:10]\n10-element Vector{BinaryHV}:\n [0, 1, 0, 0, 1, 1, 0, 1, 1, 1]\n [1, 0, 1, 0, 0, 0, 1, 1, 1, 0]\n [0, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n [0, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n [1, 1, 0, 0, 1, 0, 1, 1, 1, 0]\n [0, 0, 1, 1, 1, 1, 0, 0, 1, 0]\n [0, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n [1, 1, 1, 0, 1, 1, 0, 0, 1, 1]\n [1, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n [1, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n\njulia> bindsequence(vs)\n10-element BinaryHV:\n 0\n 1\n 1\n 0\n 1\n 1\n 0\n 1\n 1\n 1\n\nExtended help\n\nThis encoding is based on the following mathematical notation:\n\notimes_i=1^m Pi(V_i i-1)\n\nwhere V is the hypervector collection, m is the size of the hypervector collection, i is the position of the entry in the collection, and \\otimes and \\Pi are the binding and shift operations.\n\nReferences\n\nTorchhd documentation\n\nSee also\n\nbundlesequence: Bundle-sequence encoding, bundling-variant of this encoder\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.bipol2grad-Tuple{Real}","page":"API","title":"HyperdimensionalComputing.bipol2grad","text":"bipol2grad(x::Number)\n\nMaps a bipolar number in [-1, 1] to the [0, 1] interval.\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.bundlesequence-Tuple{AbstractVector{<:AbstractHV}}","page":"API","title":"HyperdimensionalComputing.bundlesequence","text":"bundlesequence(vs::AbstractVector{<:AbstractHV})\n\nBundling-based sequence. The first value is not permuted, the last value is permuted n-1 times.\n\nArguments\n\nvs::AbstractVector{<:AbstractHV}: Hypervector sequence\n\nExamples\n\njulia> vs = [BinaryHV(10) for _ in 1:10]\n10-element Vector{BinaryHV}:\n [0, 1, 0, 0, 1, 1, 0, 1, 1, 1]\n [1, 0, 1, 0, 0, 0, 1, 1, 1, 0]\n [0, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n [0, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n [1, 1, 0, 0, 1, 0, 1, 1, 1, 0]\n [0, 0, 1, 1, 1, 1, 0, 0, 1, 0]\n [0, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n [1, 1, 1, 0, 1, 1, 0, 0, 1, 1]\n [1, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n [1, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n\njulia> bundlesequence(vs)\n10-element BinaryHV:\n 0\n 1\n 0\n 0\n 0\n 0\n 0\n 0\n 1\n 1\n\nExtended help\n\nThis encoding is based on the following mathematical notation:\n\noplus_i=1^m Pi(V_i i-1)\n\nwhere V is the hypervector collection, m is the size of the hypervector collection, i is the position of the entry in the collection, and \\oplus and \\Pi are the bundling and shift operations.\n\nReferences\n\nTorchhd documentation\n\nSee also\n\nbindsequence: Binding-sequence encoding, binding-variant of this encoder\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.convertlevel-Tuple{Any, Vararg{Any}}","page":"API","title":"HyperdimensionalComputing.convertlevel","text":"convertlevel(hvlevels, numvals..., kwargs...)\n\nCreates the encoder and decoder for a level incoding in one step. See encodelevel and decodelevel for their respective documentations.\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.crossproduct-Union{Tuple{T}, Tuple{T, T}} where T<:(AbstractVector{<:AbstractHV})","page":"API","title":"HyperdimensionalComputing.crossproduct","text":"crossproduct(U::T, V::T) where {T <: AbstractVector{<:AbstractHV}}\n\nCross product between two sets of hypervectors.\n\nArguments\n\nU::AbstractVector{<:AbstractHV}: Hypervectors\nV::AbstractVector{<:AbstractHV}: Hypervectors\n\nExamples\n\njulia> us = [BinaryHV(10) for _ in 1:5]\n5-element Vector{BinaryHV}:\n [1, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n [0, 1, 0, 0, 1, 1, 1, 0, 1, 0]\n [0, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n [0, 1, 1, 0, 1, 0, 1, 1, 1, 0]\n [1, 0, 0, 1, 0, 0, 1, 1, 1, 1]\n\njulia> vs = [BinaryHV(10) for _ in 1:5]\n5-element Vector{BinaryHV}:\n [0, 1, 1, 1, 1, 1, 0, 1, 0, 0]\n [0, 0, 1, 0, 0, 1, 1, 0, 0, 1]\n [0, 0, 0, 0, 1, 0, 0, 1, 0, 1]\n [1, 0, 1, 1, 0, 1, 1, 1, 1, 1]\n [1, 0, 1, 0, 0, 1, 0, 1, 0, 1]\n\njulia> crossproduct(us, vs)\n10-element BinaryHV:\n 0\n 0\n 1\n 0\n 1\n 0\n 0\n 1\n 0\n 1\n\nExtended help\n\nThis encoding strategy first creates a multiset from both input hypervector sets, which are then bound together to generate all cross products, i.e.\n\nU₁ × V₁ + U₁ × V₂ + ... + U₁ × Vₘ + ... + Uₙ × Vₘ\n\nThis encoding is based on the following formula:\n\n(oplus_i=1^m U_i) otimes (oplus_i=1^n V_i)\n\nwhere U and V are collections of hypervectors, m and n are the sizes of the U and V collections, ì is the position in the hypervector collection, and \\oplus and \\otimes are the bundling and binding operations.\n\nReferences\n\nTorchhd documentation\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.decodelevel-Tuple{AbstractVector{<:AbstractHV}, Any}","page":"API","title":"HyperdimensionalComputing.decodelevel","text":"decodelevel(hvlevels::AbstractVector{<:AbstractHV}, numvalues)\n\nGenerate a decoding function based on level, for decoding numerical values. It returns a function that gives the numerical value for a given hypervector, based on similarity matching.\n\nArguments\n\nhvlevels::AbstractVector{<:AbstractHV}: vector of hypervectors representing the level encoding\nnumvalues: the range or vector with the corresponding numerical values\n\nExample\n\nnumvalues = range(0, 2pi, 100)\nhvlevels = level(BipolarHV(), 100)\n\ndecoder = decodelevel(hvlevels, numvalues)\n\ndecoder(hvlevels[17])  # value that closely matches the corresponding HV\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.encodelevel-Tuple{AbstractVector{<:AbstractHV}, Any}","page":"API","title":"HyperdimensionalComputing.encodelevel","text":"encodelevel(hvlevels::AbstractVector{<:AbstractHV}, numvalues; testbound=false)\n\nGenerate an encoding function based on level, for encoding numerical values. It returns a function that gives the corresponding hypervector for a given numerical input.\n\nArguments\n\nhvlevels::AbstractVector{<:AbstractHV}: vector of hypervectors representing the level encoding\nnumvalues: the range or vector with the corresponding numerical values\n[testbound=false]: optional keyword argument to check whether the provided value is in bounds\n\nExample\n\nnumvalues = range(0, 2pi, 100)\nhvlevels = level(BipolarHV(), 100)\n\nencoder = encodelevel(hvlevels, numvalues)\n\nencoder(pi/3)  # hypervector that best represents this numerical value\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.encodelevel-Tuple{AbstractVector{<:AbstractHV}, Number, Number}","page":"API","title":"HyperdimensionalComputing.encodelevel","text":"encodelevel(hvlevels::AbstractVector{<:AbstractHV}, a::Number, b::Number; testbound=false)\n\nSee encodelevel, same but provide lower (a) and upper (b) limit of the interval to be encoded.\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.grad2bipol-Tuple{Real}","page":"API","title":"HyperdimensionalComputing.grad2bipol","text":"grad2bipol(x::Number)\n\nMaps a graded number in [0, 1] to the [-1, 1] interval.\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.graph-Union{Tuple{T}, Tuple{T, T}} where T<:(AbstractVector{<:AbstractHV})","page":"API","title":"HyperdimensionalComputing.graph","text":"graph(source::T, target::T, directed::Bool = false)\n\nGraph for source-target pairs. Can be directed or undirected.\n\nArguments\n\nsource::T: Source node hypervectors\ntarget::T: Target node hypervectors\ndirected::Bool = false: Whether the graph is directed or not\n\nExample\n\nExtended help\n\nThis encoding is based on the following mathematical notation:\n\nUndirected graphs\n\notimes_i=1^m S_i otimes T_i\n\nDirected graphs\n\notimes_i=1^m S_i otimes Pi(T_i)\n\nwhere K and V are the key and value hypervector collections, m is the size of the hypervector collection, i is the position of the entry in the collection, and \\otimes, \\oplus and \\Pi are the binding, bundling and shift operations.\n\nSee also\n\nhashtable: Hash table encoding, underlying encoding strategy of this encoder.\n\nReferences\n\nTorchhd documentation\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.hashtable-Union{Tuple{T}, Tuple{T, T}} where T<:(AbstractVector{<:AbstractHV})","page":"API","title":"HyperdimensionalComputing.hashtable","text":"hashtable(keys::T, values::T) where {T <: AbstractVector{<:AbstractHV}}\n\nHash table from keys-values hypervector pairs. Keys and values must be the same length in order to encode as hypervector.\n\nArguments\n\nkeys::AbstractVector{<:AbstractHV}: Keys hypervectors\nvalues::AbstractVector{<:AbstractHV}: Values hypervectors\n\nExample\n\njulia> ks = [BinaryHV(10) for _ in 1:5]\n5-element Vector{BinaryHV}:\n [0, 0, 0, 1, 0, 1, 1, 0, 0, 0]\n [1, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n [0, 0, 0, 0, 1, 1, 1, 0, 1, 1]\n [1, 0, 0, 0, 0, 1, 1, 0, 1, 0]\n [0, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n\njulia> vs = [BinaryHV(10) for _ in 1:5]\n5-element Vector{BinaryHV}:\n [0, 1, 0, 0, 1, 1, 0, 1, 0, 0]\n [1, 0, 1, 1, 1, 0, 1, 0, 0, 0]\n [0, 0, 0, 0, 0, 1, 1, 0, 1, 0]\n [0, 1, 1, 0, 0, 0, 0, 1, 0, 1]\n [0, 1, 1, 0, 0, 0, 1, 1, 0, 1]\n\njulia> hashtable(ks, vs)\n10-element BinaryHV:\n 0\n 0\n 0\n 1\n 1\n 0\n 1\n 0\n 1\n 1\n\nExtended help\n\nThis encoding is based on the following mathematical notation:\n\noplus_i=1^m K_i otimes V_i\n\nwhere K and V are the key and value hypervector collections, m is the size of the hypervector collection, i is the position of the entry in the collection, and \\otimes and \\oplus are the binding and bundling operations.\n\nReferences\n\nTorchhd documentation\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.level-Union{Tuple{HV}, Tuple{HV, Int64}} where HV<:AbstractHV","page":"API","title":"HyperdimensionalComputing.level","text":"level(v::HV, n::Int) where {HV <: AbstractHV}\nlevel(HV::Type{<:AbstractHV}, n::Int; D::Int = 10_000)\n\nCreates a set of level correlated hypervectors, where the first and last hypervectors are quasi-orthogonal.\n\nArguments\n\nv::HV: Base hypervector\nn::Int: Number of levels (alternatively, provide a vector to be encoded)\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.multibind-Tuple{AbstractVector{<:AbstractHV}}","page":"API","title":"HyperdimensionalComputing.multibind","text":"multibind(vs::AbstractVector{<:AbstractHV})\n\nBinding of multiple hypervectors, binds all the input hypervectors together.\n\nArguments\n\nvs::AbstractVector{<:AbstractHV}: Hypervectors\n\nExamples\n\njulia> vs = [BinaryHV(10) for _ in 1:10]\n10-element Vector{BinaryHV}:\n [0, 1, 0, 0, 1, 1, 0, 1, 1, 1]\n [1, 0, 1, 0, 0, 0, 1, 1, 1, 0]\n [0, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n [0, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n [1, 1, 0, 0, 1, 0, 1, 1, 1, 0]\n [0, 0, 1, 1, 1, 1, 0, 0, 1, 0]\n [0, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n [1, 1, 1, 0, 1, 1, 0, 0, 1, 1]\n [1, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n [1, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n\njulia> multibind(vs)\n10-element BinaryHV:\n 1\n 1\n 1\n 0\n 1\n 1\n 1\n 0\n 1\n 0\n\nExtended help\n\nThis encoding is based on the following mathematical notation:\n\notimes_i=1^m V_i\n\nwhere V is the hypervector collection, m is the size of the hypervector collection, i is the position of the entry in the collection, and \\otimes is the binding operation.\n\nReferences\n\nTorchhd documentation\n\nSee also\n\nmultiset: Multiset encoding, bundling-variant of this encoder\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.multiset-Union{Tuple{AbstractVector{<:T}}, Tuple{T}} where T<:AbstractHV","page":"API","title":"HyperdimensionalComputing.multiset","text":"multiset(vs::AbstractVector{<:T})::T where {T <: AbstractHV}\n\nMultiset of input hypervectors, bundles all the input hypervectors together.\n\nArguments\n\nvs::AbstractVector{<:AbstractHV}: Hypervectors\n\nExample\n\njulia> vs = [BinaryHV(10) for _ in 1:10]\n10-element Vector{BinaryHV}:\n [0, 1, 0, 0, 1, 1, 0, 1, 1, 1]\n [1, 0, 1, 0, 0, 0, 1, 1, 1, 0]\n [0, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n [0, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n [1, 1, 0, 0, 1, 0, 1, 1, 1, 0]\n [0, 0, 1, 1, 1, 1, 0, 0, 1, 0]\n [0, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n [1, 1, 1, 0, 1, 1, 0, 0, 1, 1]\n [1, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n [1, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n\njulia> multiset(vs)\n10-element BinaryHV:\n 0\n 1\n 0\n 0\n 1\n 0\n 0\n 0\n 1\n 0\n\nExtended help\n\nThis encoding is based on the following mathematical notation:\n\noplus_i=1^m V_i\n\nwhere V is the hypervector collection, m is the size of the hypervector collection, i is the position of the entry in the collection, and \\oplus is the bundling operation.\n\nReferences\n\nTorchhd documentation\n\nSee also\n\nmultibind: Multibind encoding, binding-variant of this encoder\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.nearest_neighbor-Tuple{AbstractHV, Any, Int64}","page":"API","title":"HyperdimensionalComputing.nearest_neighbor","text":"nearest_neighbor(u::AbstractHV, collection[, k::Int]; kwargs...)\n\nReturns the element of collection that is most similar to u. \n\nFunction outputs (τ, i, xi) with τ the highest similarity value, i the index (or key if collection is a dictionary) of the closest  neighbor and xi the closest vector. kwargs is an optional argument for the similarity search.\n\nIf a number k is given, the k closest neighbor are returned, as a sorted list of (τ, i).\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.ngrams","page":"API","title":"HyperdimensionalComputing.ngrams","text":"ngrams(vs::AbstractVector{<:AbstractHV}, n::Int = 3)\n\nCreates a hypervector with the n-gram statistics of the input.\n\nArguments\n\nvs::AbstractVector{<:AbstractHV}: Hypervector collection\nn::Int = 3: n-gram size\n\nExamples\n\njulia> vs = [BinaryHV(10) for _ in 1:10]\n10-element Vector{BinaryHV}:\n [0, 1, 0, 0, 1, 0, 1, 0, 0, 1]\n [0, 0, 1, 1, 1, 0, 0, 1, 1, 1]\n [0, 0, 1, 0, 0, 1, 1, 1, 0, 0]\n [1, 0, 0, 0, 1, 0, 0, 1, 1, 1]\n [0, 1, 0, 0, 1, 0, 1, 1, 0, 0]\n [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n [1, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n [0, 1, 0, 1, 0, 0, 0, 1, 1, 0]\n [0, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n [1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n\njulia> ngrams(vs)\n10-element BinaryHV:\n 1\n 1\n 1\n 1\n 1\n 1\n 0\n 1\n 0\n 1\n\nExtended help\n\nThis encoding is defined by the following mathematical notation:\n\noplus_i=1^m-notimes_j=1^n-1Pi^n-j-1(V_i+j)\n\nwhere V is the collection of hypervectors, m is the number of hypervectors in the collection V, n is the window size, i is the position in the sequence, j is the position in the n-gram, and \\oplus, \\otimes and \\Pi are the bundling, binding and shift operations.\n\nnote: Note\nFor n = 1 use multiset() instead\nFor n = m use bindsequence() instead\n\nSee also\n\nmultiset: Multiset encoding, equivalent to ngram(vs, 1)\nbindsequence: Bind-sequence encoding, equivalent to ngram(vs, length(vs))\n\nReferences\n\nTorchhd documentation\n\n\n\n\n\n","category":"function"},{"location":"api/#HyperdimensionalComputing.similarity-Tuple{AbstractHV}","page":"API","title":"HyperdimensionalComputing.similarity","text":"similarity(u::AbstractHV; [method])\n\nCreate a function that computes the similarity between its argument and uusingsimilarity, i.e. a function equivalent tov -> similarity(u, v)`.\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.similarity-Tuple{AbstractVector, AbstractVector}","page":"API","title":"HyperdimensionalComputing.similarity","text":"similarity(u::AbstractVector, v::AbstractVector; method::Symbol)\n\nComputes similarity between two (hyper)vectors using a method ∈ [:cosine, :jaccard, :hamming]. When no method is given, a default is used (cosine for vectors that can have negative elements and Jaccard for those that only have positive elements).\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.similarity-Tuple{AbstractVector{<:AbstractHV}}","page":"API","title":"HyperdimensionalComputing.similarity","text":"similarity(hvs::AbstractVector{<:AbstractHV}; [method])\n\nComputes the similarity matrix for a vector of hypervectors using the similarity metrics defined by the pairwise version of similarity.\n\n\n\n\n\n","category":"method"},{"location":"api/#HyperdimensionalComputing.δ","page":"API","title":"HyperdimensionalComputing.δ","text":"δ(u::AbstractHV, v::AbstractHV; [method])\nδ(u::AbstractHV; [method])\nδ(hvs::AbstractVector{<:AbstractHV}; [method])\n\nAlias for similarity. See similarity for the main documentation.\n\n\n\n\n\n","category":"function"},{"location":"api/#HyperdimensionalComputing.BinaryHV","page":"API","title":"HyperdimensionalComputing.BinaryHV","text":"BinaryHV\n\nA ternary hypervector type based on the Binary Splatter Code (BSC) vector symbolic architecture (Kanerva, 1994; Kanerva, 1995; Kanerva, 1996; Kanerva, 1997).\n\nRepresents a hypervector boolean elements, i.e. (false, true).\n\nExtended help\n\nReferences\n\nKanerva, P. (1994). The Spatter Code for Encoding Concepts at Many Levels. In International Conference on Artificial Neural Networks (ICANN), pages 226–229.\nKanerva, P. (1995). A Family of Binary Spatter Codes. In International Conference on Artificial Neural Networks (ICANN), pages 517–522.\nKanerva, P. (1996). Binary Spatter-Coding of Ordered K-tuples. In International Conference on Artificial Neural Networks (ICANN), volume 1112 of Lecture Notes in Computer Science, pages 869–873.\nKanerva, P. (1997). Fully Distributed Representation. In Real World Computing Symposium (RWC), pages 358–365.\n\n\n\n\n\n","category":"type"},{"location":"api/#HyperdimensionalComputing.TernaryHV","page":"API","title":"HyperdimensionalComputing.TernaryHV","text":"TernaryHV\n\nA ternary hypervector type based on the Multiply-Add-Permute (MAP) vector symbolic architecture (Gayler, 1998).\n\nRepresents a hypervector with elements in (-1, 1).\n\nExtended help\n\nReferences\n\nGayler, R. W. (1998). Multiplicative Binding, Representation Operators & Analogy. In Advances in Analogy Research: Integration of Theory and Data from the Cognitive, Computational, and Neural Sciences, pages 1–4.\n\n\n\n\n\n","category":"type"},{"location":"examples/introduction-to-hdc/","page":"Introduction to HDC","title":"Introduction to HDC","text":"using Handcalcs #hide","category":"section"},{"location":"examples/introduction-to-hdc/#Introduction","page":"Introduction to HDC","title":"Introduction","text":"Hyperdimensional Computing (HDC) is a brain-inspired computational paradigm that represents and manipulates information using high-dimensional vectors called hypervectors. These vectors typically have thousands of dimensions (often 1,000-10,000), making them \"hyperdimensional.\"\n\nThe key insight is that high-dimensional spaces have unique mathematical properties that allow for robust, fault-tolerant computation.\n\nLet's start by loading the package in question, as follows:\n\nusing HyperdimensionalComputing","category":"section"},{"location":"examples/introduction-to-hdc/#Creating-hypervectors","page":"Introduction to HDC","title":"Creating hypervectors","text":"First, we will create a random bipolar hypervector. This is done as follows:\n\nBipolarHV()\n\nAs you may see, by default the hypervector created has 10.000 dimensions. This is the default value in HyperdimensionalComputing.jl, but one can can create a hypervector of any given dimensionality by providing the size of this as an argument:\n\nBipolarHV(; D = 8)\n\nAlternatively, one can create a hypervector directly from a Vector{T} where {T} is an appropiate data type, e.g. integers for BipolarHV:\n\nBipolarHV(rand((-1, 1), 8))\n\nor you can directly pass any Julia structure to use it as a seed for the hypervector generation:\n\nBipolarHV(:foo)\n\nLet's create 3 bipolar hypervector to use for the tutorial:\n\nh₁ = BipolarHV(; D = 8)\nh₂ = BipolarHV(; D = 8)\nh₃ = BipolarHV(; D = 8);\nnothing #hide\n\nThe package has different hypervector types, such as BipolarHV, TernaryHV, RealHV, GradedBipolarHV, and GradedHV. All of this hypervectors have a common abstract type AbstractHV which can be used to build additional functions or encoding strategies (more on both later).\n\ninfo: On (abstract) types\nAll hypervectors implemented on HyperdimensionalComputing.jl can be found by checking the docstrings for the AbstractHV (by typing ?AbstractHV on the Julia REPL).For more information on a specific hypervector type, the docstrings contain information on the implementation, operations, similarity measurement and other technical characteristics.","category":"section"},{"location":"examples/introduction-to-hdc/#Fundamental-operations-with-hypervectors","page":"Introduction to HDC","title":"Fundamental operations with hypervectors","text":"HDC uses three primary operations that preserve the hyperdimensional properties and allow for the representation more complex structures:","category":"section"},{"location":"examples/introduction-to-hdc/#Bundling","page":"Introduction to HDC","title":"Bundling","text":"Bundling (also known as superposition) combines multiple hypervectors to create a new hypervector that is similar to it's constituyents.\n\nu = h_1 + h_2 + h_3\n\nwhere  denotes a potential normalization operations. In the case of bipolar hypervectors, this normalization operation is the sign function, which is defined as follows:\n\ntextsign(i) = begincases\n  +1  textif  i  0 \n  -1  textif  i  0 \n   0  textotherwise \nendcases\n\nIn HyperdimensionalComputing.jl, you can bundle hypervectors as follows:\n\nbundle([h₁, h₂, h₃])\n\nalternatively, you can use the + operator (which if overloaded for all AbstractHV):\n\nh₁ + h₂ + h₃\n\nThis operation generates a hypervector that is similar to all it's contituyent hypervectors, such that\n\nh₁ sim u h₂ sim u h₃ sim u\n\nwhere sim means that the hypervectors are similar, i.e. they share more components than expected by chance.","category":"section"},{"location":"examples/introduction-to-hdc/#Binding","page":"Introduction to HDC","title":"Binding","text":"Binding combines multiple hypervectors to create a new hypervector that is dissimilar to it's constituyents, such that:\n\nv = h₁ times h₂ times h₃\n\nwhere  represents a normalization procedure.\n\nIn HyperdimensionalComputing.jl, you can bind hypervectors as follows:\n\nbind([h₁, h₂, h₃])\n\nalternatively, you can use the * operator (which if overloaded for all AbstractHV):\n\nh₁ * h₂ * h₃\n\nThis operation generates a hypervector that is similar to all it's contituyent hypervectors, such that\n\nh₁ nsim v h₂ nsim v h₃ nsim v\n\nwhere nsim means that the hypervectors are dissimilar, i.e. they are quasi-orthogonal.","category":"section"},{"location":"examples/introduction-to-hdc/#Permutation","page":"Introduction to HDC","title":"Permutation","text":"Permutation (also known as shifting) is a special case of binding that creates a variant of a single hypervector via, generally speaking, a circular vector shifting with one or more positions.\n\nm = rho(h₁)\n\nh₄ = TernaryHV(collect(0:9))\nh₄.v\n\nρ(h₄).v\n\nThe new hypervector will be, in principle, dissimilar to it's original version, such that:\n\nh_1 nsim rho(h_1) nsim rhorho(h_1) nsim rhorho(h_1) \n\nwhere nsim means that the hypervectors are dissimilar, i.e. they are quasi-orthogonal.\n\nIn HyperdimensionalComputing.jl, one can shift hypervector as follows:\n\nρ(h₁, 1)\n\nh₁ != ρ(h₁, 1) != ρ(h₁, 2) != ρ(h₁, 3)","category":"section"},{"location":"examples/introduction-to-hdc/#Similarity","page":"Introduction to HDC","title":"Similarity","text":"Althought technically not an operation, in order to retrieve information from hypervectors, we need to compare them using similarity/distance functions. HyperdimensionalComputing.jl provides a handy similarity function that accepts:\n\n2 hypervectors:\n\nsimilarity(h₁, h₂)\n\nA vector of hypervectors:\n\nsimilarity(h₁, h₁)\n\nor a hypervector and a vector of hypervectors:\n\nsimilarity.(Ref(h₁), [h₁, h₂, h₃])\n\nδ is a synonim of similarity, and can also be used to create a function for similarity comparison, e.g.\n\nf = δ(h₁)\n\nf.([h₁, h₂, h₃])","category":"section"},{"location":"examples/introduction-to-hdc/#Encoding-things-as-hypervectors","page":"Introduction to HDC","title":"Encoding things as hypervectors","text":"The true power of HDC emerges when we combine the fundamental operations to encode complex data structures as hypervectors. By creatively applying bundling, binding, and shifting, we can represent virtually any type of information - from sequences and hierarchies to graphs and associative memories. The operations act as building blocks that can be composed in countless ways, limited only by our imagination and the specific requirements of our application. Let's explore some fundamental encoding strategies that demonstrate this flexibility.","category":"section"},{"location":"examples/introduction-to-hdc/#Key-value-pairs","page":"Introduction to HDC","title":"Key-value pairs","text":"Animal hypervectors:\n\nH_dog = TernaryHV(:dog)\nH_cat = TernaryHV(:cat)\nH_cow = TernaryHV(:cow)\nH_animals = [H_dog, H_cat, H_cow]\n\nSound hypervectors:\n\nH_bark = TernaryHV(:bark)\nH_meow = TernaryHV(:meow)\nH_moo = TernaryHV(:moo)\nH_sounds = [H_bark, H_meow, H_moo]\n\nAssociative memory:\n\nmemory = (H_dog * H_bark) + (H_cat * H_meow) + (H_cow * H_moo);\nnothing #hide\n\nnote: Note\n\n\n#\t  Alternatively you can use the `hashtable` encoder to achieve the same:\n\nmemory == hashtable(H_animals, H_sounds)\n\nQuerying memory to search for dog's sound:\n\nnearest_neighbor(H_dog * memory, H_sounds)\n\nQuerying memory to search which animals go \"moo\":\n\nnearest_neighbor(H_moo * memory, H_animals)\n\nThis is a very simple example, but you could think of having a more complex thing going on or having more animals that, for example, share sounds.","category":"section"},{"location":"examples/introduction-to-hdc/#Sequences","page":"Introduction to HDC","title":"Sequences","text":"N-grams represent sequences by encoding the order of elements. This is particularly useful for text processing where word order matters.\n\nEncode the phrases using the builtin ngrams encoder, with uses a sliding window of 3 characters.\n\nLet's encode some phrases and then search for a specific word in them. First, the sentences list:\n\nphrases = [\n    \"the quick brown fox jumps over the lazy dog\",\n    \"the slick grown box bumps under the hazy fog\",\n    \"the thick known cox dumps inter the crazy cog\",\n    \"the brick shown pox lumps enter the glazy jog\",\n    \"the stick blown sox pumps winter the blazy log\",\n];\nnothing #hide\n\nNow, lets encode sentences using the characters as seed for our basis hypervectors and use n-gram encoding to represent the sentences as hypervectors:\n\nencode(p::String) = map(c -> BinaryHV(c), collect(p)) |> ngrams\n\nH_phrases = map(encode, phrases)\n\nNow that we have the sentence hypervectors, let's search for \"crazy\" in phrases:\n\nquery = map(c -> BinaryHV(c), collect(\"crazy\")) |> ngrams\n\nnearest_neighbor(query, H_phrases)\n\nGreat! We correctly found that \"crazy\" is in phrase 3.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/whats-the-dollar-of-mexico/#Replicating-\"What’s-the-Dollar-of-Mexico?\"-in-HyperdimensionalComputing.jl","page":"What's the Dollar of Mexico?","title":"Replicating \"What’s the Dollar of Mexico?\" in HyperdimensionalComputing.jl","text":"Kanerva showcases in this publication how we can create prototype concepts and mappings between them using Hyperdimensional Computing (HDC), a computing paradigm based on very big random vectors populated with binary values.\n\nThis very big random vectors, which we calls \"words\" or \"codes\", represent concepts, but when analyzing this as mathematical constructs they are nothing else than random signals. When combined using bundling and binding operations, one can construct concepts and reason based on the similarity of this words, always assuming that this retrieval is approximated.\n\nSince the properties of the operations that enable composability of this \"codes\" are mathematically grounded, the characteristics of the operations permeate into this modelling mindset and enable for the creation of (i) prototype representations and (ii) mapping function, both based on the same underlying mathematical structure. Thanks to this, we can either model mapping one concept into another space or directly map a mapping function from one space into another, basically mimicking how the brain works:\n\nThe literal image created by the words is intended to be transferred to and interpreted in a\n\nnew context, exemplifying the mind's reliance on prototypes: the literal meaning provides the   prototype. When the mind expands its scope in this way it creates an incredibly rich web of   associations and meaning...\n\nTo showcase this, we will reimplement the example proposed by Kanerva on this paper.\n\nusing HyperdimensionalComputing\n\nIn HDC, concepts are represented as high-dimensional random vectors (hypervectors). Here, we create hypervectors for the abstract concepts of COUNTRY, CAPITAL, and MONEY.\n\nCOUNTRY = BipolarHV(:country)\nCAPITAL = BipolarHV(:capital)\nMONEY = BipolarHV(:money)\n\nCOUNTRY, CAPITAL, MONEY\n\nNext, we create hypervectors for specific instances: countries, their capitals, and currencies.\n\nUSA = BipolarHV(:usa)\nMEX = BipolarHV(:mexico)\n\nWDC = BipolarHV(:wdc)      # Washington DC\nMXC = BipolarHV(:mxc)      # Mexico City\n\nDOL = BipolarHV(:dollar)\nPES = BipolarHV(:peso)\n\nUSA, MEX, WDC, MXC, DOL, PES\n\nWe now build holistic representations for the United States and Mexico by binding each concept (country, capital, money) to its specific instance and then adding them together.\n\nUSTATES = (COUNTRY * USA) + (CAPITAL * WDC) + (MONEY * DOL)\nMEXICO = (COUNTRY * MEX) + (CAPITAL * MXC) + (MONEY * PES)\n\nUSTATES, MEXICO\n\nThese composite hypervectors encode all the information about each country in a single vector. The Base.isapprox function or ≈ operator checks if two hypervectors are approximately equal (i.e., similar).\n\nUSTATES ≈ MEXICO\n\nBy binding (represented by the * operator) the holistic representations of USA and Mexico, we create a new hypervector that encodes the relationships between their respective elements: USA with Mexico, Washington DC with Mexico City, and dollar with peso, plus some noise due to the high-dimensional operations.\n\nF_UM = USTATES * MEXICO\n\nF_UM ≈ (USA * MEX) + (WDC * MXC) + (DOL * PES)\n\nTo answer Kanerva's question \"What in Mexico corresponds to United States' dollar?\", we unbind (represented again by the * operator) the dollar hypervector with the USA-Mexico relationship vector. The result should be similar to the peso hypervector.\n\nDOL * F_UM ≈ PES\n\nLet's add another country, Sweden, with its capital and currency.\n\nSWE = BipolarHV(:sweden)\nSTO = BipolarHV(:stockholm)\nSEK = BipolarHV(:krona)\n\nSWEDEN = (COUNTRY * SWE) + (CAPITAL * STO) + (MONEY * SEK)\n\nWe can now explore more complex relationships by binding and combining these holistic representations. For example, F_SUencodes the relationship between Sweden and USA, and F_SM between Sweden and Mexico.\n\nF_UM = USTATES * MEXICO\nF_SU = SWEDEN * USTATES\nF_SM = SWEDEN * MEXICO\n\nCombining these relationship vectors allows us to infer new relationships, such as how Sweden relates to Mexico via the USA.\n\nF_SU * F_UM ≈ F_SM\n\nWe can also directly query for corresponding elements between countries:\n\nFor example, what is the currency of Mexico, given the currency of the USA?\n\nUSTATES * DOL ≈ MEXICO * PES\n\nOr, what is the currency of Mexico, given the USA and its currency?\n\nUSTATES * DOL * MEXICO ≈ PES\n\nSimilarly, we can query for Sweden's currency using the same approach.\n\nUSTATES * DOL * MEXICO ≈ SEK\n\nOr, recover the original currency of the USA.\n\nUSTATES * DOL * MEXICO ≈ DOL\n\nThis tutorial demonstrates how hyperdimensional computing enables analogical reasoning and flexible querying by representing and manipulating concepts as high-dimensional vectors.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#HyperdimensionalComputing.jl","page":"HyperdimensionalComputing.jl","title":"HyperdimensionalComputing.jl","text":"Documentation for HyperdimensionalComputing.","category":"section"},{"location":"#Overview","page":"HyperdimensionalComputing.jl","title":"Overview","text":"HyperdimensionalComputing.jl provides...","category":"section"},{"location":"#Manual","page":"HyperdimensionalComputing.jl","title":"Manual","text":"Pages = [\n    \"api.md\",\n    \"examples.md\"\n]\nDepth = 2","category":"section"},{"location":"#Index","page":"HyperdimensionalComputing.jl","title":"Index","text":"Pages = [\"index.md\"]","category":"section"},{"location":"developers/#Developer-guide","page":"Developer guide","title":"Developer guide","text":"","category":"section"},{"location":"developers/#Formatting","page":"Developer guide","title":"Formatting","text":"We use Runic.jl to format the HyperdimensionalComputing.jl codebase. We recommend installing pre-commit to automatically set up pre-commit hooks that check code formatting. To install the pre-commit hooks, run pre-commit install in the root directory. After installation, the hooks will run automatically on each commit.\n\nSince implementing comprehensive development tooling can be challenging, pull requests that don't follow Runic.jl formatting will be automatically revised with review comments indicating the required changes. We strongly encourage contributors to format their code before submitting pull requests, as large unformatted PRs can be difficult to review and may cause significant delays in the review process.","category":"section"}]
}
