<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Introduction to HDC · HyperdimensionalComputing.jl</title><meta name="title" content="Introduction to HDC · HyperdimensionalComputing.jl"/><meta property="og:title" content="Introduction to HDC · HyperdimensionalComputing.jl"/><meta property="twitter:title" content="Introduction to HDC · HyperdimensionalComputing.jl"/><meta name="description" content="Documentation for HyperdimensionalComputing.jl."/><meta property="og:description" content="Documentation for HyperdimensionalComputing.jl."/><meta property="twitter:description" content="Documentation for HyperdimensionalComputing.jl."/><meta property="og:url" content="https://cvigilv.github.io/HyperdimensionalComputing.jl/examples/introduction-to-hdc/"/><meta property="twitter:url" content="https://cvigilv.github.io/HyperdimensionalComputing.jl/examples/introduction-to-hdc/"/><link rel="canonical" href="https://cvigilv.github.io/HyperdimensionalComputing.jl/examples/introduction-to-hdc/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">HyperdimensionalComputing.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HyperdimensionalComputing.jl</a></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>Introduction to HDC</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#1.-Hypervector-Creation"><span>1. Hypervector Creation</span></a></li><li><a class="tocitem" href="#2.-Fundamental-Operations"><span>2. Fundamental Operations</span></a></li><li><a class="tocitem" href="#3.-Encoding-Data-as-Hypervectors"><span>3. Encoding Data as Hypervectors</span></a></li><li class="toplevel"><a class="tocitem" href="#Create-n-gram-encoding-for-a-sequence"><span>Create n-gram encoding for a sequence</span></a></li><li class="toplevel"><a class="tocitem" href="#Create-and-query-HDC-hash-table"><span>Create and query HDC hash table</span></a></li><li class="toplevel"><a class="tocitem" href="#Query-the-hash-table"><span>Query the hash table</span></a></li><li><a class="tocitem" href="#4.-Basic-Learning-via-Prototype-Generation"><span>4. Basic Learning via Prototype Generation</span></a></li><li class="toplevel"><a class="tocitem" href="#Example-with-simulated-language-data"><span>Example with simulated language data</span></a></li><li class="toplevel"><a class="tocitem" href="#Train-prototypes-for-each-language"><span>Train prototypes for each language</span></a></li><li class="toplevel"><a class="tocitem" href="#Classify-new-text"><span>Classify new text</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Introduction to HDC</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Introduction to HDC</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/cvigilv/HyperdimensionalComputing.jl/blob/main/docs/src/examples/introduction-to-hdc.jl#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Introduction-to-Hyperdimensional-Computing-with-HyperdimensionalComputing.jl"><a class="docs-heading-anchor" href="#Introduction-to-Hyperdimensional-Computing-with-HyperdimensionalComputing.jl">Introduction to Hyperdimensional Computing with HyperdimensionalComputing.jl</a><a id="Introduction-to-Hyperdimensional-Computing-with-HyperdimensionalComputing.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction-to-Hyperdimensional-Computing-with-HyperdimensionalComputing.jl" title="Permalink"></a></h1><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>Hyperdimensional Computing (HDC) is a brain-inspired computational paradigm that represents and manipulates information using high-dimensional vectors called <strong>hypervectors</strong>. These vectors typically have thousands of dimensions (often 1,000-10,000), making them &quot;hyperdimensional.&quot;</p><p>The key insight is that high-dimensional spaces have unique mathematical properties that allow for robust, fault-tolerant computation. Think of it like having a vast library where each book (hypervector) has so many pages that even if you change a few pages, you can still identify which book it is.</p><p>Rather than the specific choice of the values in the hyperdimensional representations, we identify 4 hallmarks that distinguish hyperdimensional computing from other approaches:</p><ol><li><strong>Hyperdimensional</strong>: The HVs live in a very high-dimensional space, large enough such that</li></ol><p>random components can be seen as distinct and dissimilar from one another;</p><ol><li><strong>Homogeneous</strong>: The vast majority of HVs all have highly similar properties: they have</li></ol><p>(approximately) the same norm, are all equally (dis)similar to one another, and have the same dimensionality, even if they embed more complex information;</p><ol><li><strong>Holographic</strong>: The information encoded in an HV is distributed over its many dimensions;</li></ol><p>no specific region is more informative than another for a specific piece of information;</p><ol><li><strong>Robust</strong>: Randomly changing a modest number of the components does not substantially change</li></ol><p>an HV&#39;s meaning.</p><p>The basic operations needed for HDC are remarkably simple. They hinge on 4 operations to manipulate and extract the information in the HVs:</p><ul><li>Generating new HVs from scratch;</li><li>Combining a set of HVs into a new HV that is similar to all;</li><li>Using one or more HVs to generate a new one that is dissimilar to its parent(s);</li><li>Comparing 2 HVs to detect whether they are more (dis)similar than expected by chance.</li></ul><h2 id="1.-Hypervector-Creation"><a class="docs-heading-anchor" href="#1.-Hypervector-Creation">1. Hypervector Creation</a><a id="1.-Hypervector-Creation-1"></a><a class="docs-heading-anchor-permalink" href="#1.-Hypervector-Creation" title="Permalink"></a></h2><h3 id="Bipolar-Hypervectors"><a class="docs-heading-anchor" href="#Bipolar-Hypervectors">Bipolar Hypervectors</a><a id="Bipolar-Hypervectors-1"></a><a class="docs-heading-anchor-permalink" href="#Bipolar-Hypervectors" title="Permalink"></a></h3><p>In this tutorial, we&#39;ll focus on <strong>bipolar hypervectors</strong> - vectors containing only -1s and +1s. A typical bipolar hypervector has dimension <span>$D$</span> (e.g., <span>$D = 10,000$</span>) where each element is randomly assigned:</p><p class="math-container">\[\mathbf{h} \in \{-1,+1\}^D\]</p><p>For example, a 8-dimensional bipolar hypervector might look like: <span>$\mathbf{h} = [+1, -1, +1, +1, -1, -1, +1, -1]$</span></p><p>To generate a bipolar hypervector with HyperdimensionalComputing.jl package, you can do the following:</p><pre><code class="language-julia hljs">using HyperdimensionalComputing

h = BipolarHDV()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10000-element BipolarHDV:
  1
  1
 -1
  1
  1
 -1
 -1
  1
 -1
  1
  ⋮
  1
 -1
 -1
 -1
 -1
 -1
  1
 -1
 -1</code></pre><p>This will create a random vector of -1s and +1s of 10.000 dimensions.</p><h3 id="Key-Properties"><a class="docs-heading-anchor" href="#Key-Properties">Key Properties</a><a id="Key-Properties-1"></a><a class="docs-heading-anchor-permalink" href="#Key-Properties" title="Permalink"></a></h3><p><strong>Quasi-orthogonality</strong>: Randomly generated bipolar hypervectors are approximately orthogonal to each other. In high dimensions, the probability that two random bipolar vectors are similar becomes vanishingly small - like finding two identical snowflakes.</p><p><strong>Distance Preservation</strong>: The cosine similarity between random bipolar hypervectors follows a normal distribution centered around 0.</p><h2 id="2.-Fundamental-Operations"><a class="docs-heading-anchor" href="#2.-Fundamental-Operations">2. Fundamental Operations</a><a id="2.-Fundamental-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#2.-Fundamental-Operations" title="Permalink"></a></h2><p>HDC uses three primary operations that preserve the hyperdimensional properties:</p><h3 id="2.1-Bundling-(Superposition)"><a class="docs-heading-anchor" href="#2.1-Bundling-(Superposition)">2.1 Bundling (Superposition)</a><a id="2.1-Bundling-(Superposition)-1"></a><a class="docs-heading-anchor-permalink" href="#2.1-Bundling-(Superposition)" title="Permalink"></a></h3><p><strong>Bundling</strong> combines multiple hypervectors to create a summary vector that contains information from all inputs. Think of it as creating a &quot;blend&quot; that retains traces of all ingredients.</p><p>For bipolar vectors, bundling uses <strong>majority voting</strong>:</p><p class="math-container">\[\text{Bundle}(\mathbf{h_1}, \mathbf{h_2}, ..., \mathbf{h_n})_i = \begin{cases}
+1 &amp; \text{if } \sum_{j=1}^{n} h_{j,i} &gt; 0 \\
-1 &amp; \text{otherwise}
\end{cases}\]</p><p><strong>Example</strong>: Bundling three 8-dimensional vectors:</p><ul><li><p class="math-container">\[\mathbf{a} = [+1, -1, +1, +1, -1, -1, +1, -1]\]</p></li><li><p class="math-container">\[\mathbf{b} = [-1, +1, +1, -1, +1, -1, +1, +1]\]</p></li><li><p class="math-container">\[\mathbf{c} = [+1, +1, -1, +1, -1, +1, +1, -1]\]</p></li></ul><p>Result: <span>$\text{Bundle}(\mathbf{a}, \mathbf{b}, \mathbf{c}) = [+1, +1, +1, +1, -1, -1, +1, -1]$</span></p><p>In HyperdimensionalComputing.jl, this is done as follows:</p><pre><code class="language-julia hljs">h₁ = BipolarHDV([ 1, -1,  1,  1, -1, -1,  1, -1])
h₂ = BipolarHDV([-1,  1,  1, -1,  1, -1,  1,  1])
h₃ = BipolarHDV([ 1,  1, -1,  1, -1,  1,  1, -1])
aggregate([h₁, h₂, h₃])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">8-element BipolarHDV:
  1
  1
  1
  1
 -1
 -1
  1
 -1</code></pre><p>You can also do the same using the <code>+</code> operator:</p><pre><code class="language-julia hljs">h₁ + h₂ + h₃</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">8-element BipolarHDV:
  1
  1
  1
  1
 -1
 -1
  1
 -1</code></pre><h3 id="2.2-Binding"><a class="docs-heading-anchor" href="#2.2-Binding">2.2 Binding</a><a id="2.2-Binding-1"></a><a class="docs-heading-anchor-permalink" href="#2.2-Binding" title="Permalink"></a></h3><p><strong>Binding</strong> creates associations between hypervectors, similar to forming key-value pairs. The bound result is dissimilar to both inputs, but can be &quot;unbound&quot; using one input to retrieve the other.</p><p>For bipolar vectors, binding uses <strong>element-wise multiplication</strong>:</p><p class="math-container">\[\text{Bind}(\mathbf{a}, \mathbf{b}) = \mathbf{a} \odot \mathbf{b}\]</p><p><strong>Example</strong>:</p><ul><li><p class="math-container">\[\mathbf{a} = [+1, -1, +1, +1, -1, -1, +1, -1]\]</p></li><li><p class="math-container">\[\mathbf{b} = [-1, +1, +1, -1, +1, -1, +1, +1]\]</p></li><li><p class="math-container">\[\mathbf{a} \odot \mathbf{b} = [-1, -1, +1, -1, -1, +1, +1, -1]\]</p></li></ul><p>In HyperdimensionalComputing.jl, binding can be done using the <code>bind</code> function: <code>*</code> operators:</p><pre><code class="language-julia hljs">bound = bind([h₁, h₂])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">8-element BipolarHDV:
 -1
 -1
  1
 -1
 -1
  1
  1
 -1</code></pre><p>or the <code>*</code> operator:</p><pre><code class="language-julia hljs">h₁ * h₂</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">8-element BipolarHDV:
 -1
 -1
  1
 -1
 -1
  1
  1
 -1</code></pre><p><strong>Unbinding</strong>: Since element-wise multiplication is its own inverse for bipolar vectors, <span>$(\mathbf{a} \odot \mathbf{b}) \odot \mathbf{a} = \mathbf{b}$</span>, we can unbind information from a hypervector as follows:</p><pre><code class="language-julia hljs">retrieved = bound * h₁</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">8-element BipolarHDV:
 -1
  1
  1
 -1
  1
 -1
  1
  1</code></pre><pre><code class="language-julia hljs">h₂ == retrieved</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><h3 id="2.3-Shifting/Permutation"><a class="docs-heading-anchor" href="#2.3-Shifting/Permutation">2.3 Shifting/Permutation</a><a id="2.3-Shifting/Permutation-1"></a><a class="docs-heading-anchor-permalink" href="#2.3-Shifting/Permutation" title="Permalink"></a></h3><p><strong>Shifting</strong> (or permutation) creates new hypervectors that are orthogonal to the original while preserving structure. It&#39;s like rearranging the elements according to a fixed pattern.</p><p>For bipolar vectors, a common approach is <strong>circular shifting</strong>:</p><p class="math-container">\[\text{Shift}(\mathbf{h}, k)_i = h_{(i-k) \bmod D}\]</p><p><strong>Example</strong> with right shift by 2:</p><ul><li>Original: <span>$[+1, -1, +1, +1, -1, -1, +1, -1]$</span></li><li>Shifted: <span>$[+1, -1, +1, -1, +1, +1, -1, -1]$</span></li></ul><p>In HyperdimensionalComputing.jl, this is done with the <code>Π</code> (<code>\Pi</code>) operator:</p><pre><code class="language-julia hljs">Π(h₁, 2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">8-element BipolarHDV:
  1
 -1
  1
 -1
  1
  1
 -1
 -1</code></pre><h3 id="2.4-Similarity-Measurement"><a class="docs-heading-anchor" href="#2.4-Similarity-Measurement">2.4 Similarity Measurement</a><a id="2.4-Similarity-Measurement-1"></a><a class="docs-heading-anchor-permalink" href="#2.4-Similarity-Measurement" title="Permalink"></a></h3><p>For classification and retrieval, we measure similarity using <strong>cosine similarity</strong> for bipolar vectors:</p><p class="math-container">\[\text{Similarity}(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a} \cdot \mathbf{b}}{|\mathbf{a}||\mathbf{b}|} = \frac{\mathbf{a} \cdot \mathbf{b}}{D}\]</p><p>Since bipolar vectors have unit magnitude (<span>$|\mathbf{h}| = \sqrt{D}$</span>), the cosine similarity simplifies to the normalized dot product:</p><p class="math-container">\[\text{Similarity}(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a} \cdot \mathbf{b}}{D}\]</p><pre><code class="language-julia hljs">sim = similarity(h₁, h₂)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-0.24999999999999994</code></pre><h2 id="3.-Encoding-Data-as-Hypervectors"><a class="docs-heading-anchor" href="#3.-Encoding-Data-as-Hypervectors">3. Encoding Data as Hypervectors</a><a id="3.-Encoding-Data-as-Hypervectors-1"></a><a class="docs-heading-anchor-permalink" href="#3.-Encoding-Data-as-Hypervectors" title="Permalink"></a></h2><p>The true power of HDC emerges when we combine the fundamental operations to encode complex data structures as hypervectors. By creatively applying bundling, binding, and shifting, we can represent virtually any type of information - from sequences and hierarchies to graphs and associative memories. The operations act as building blocks that can be composed in countless ways, limited only by our imagination and the specific requirements of our application. Let&#39;s explore two fundamental encoding strategies that demonstrate this flexibility.</p><h3 id="3.1-N-gram-Encoding"><a class="docs-heading-anchor" href="#3.1-N-gram-Encoding">3.1 N-gram Encoding</a><a id="3.1-N-gram-Encoding-1"></a><a class="docs-heading-anchor-permalink" href="#3.1-N-gram-Encoding" title="Permalink"></a></h3><p><strong>N-grams</strong> represent sequences by encoding the order of elements. This is particularly useful for text processing where word order matters.</p><p><strong>Approach</strong>:</p><ol><li>Assign a unique hypervector to each symbol</li><li>Use shifting to represent position</li><li>Bundle all positions to create the n-gram</li></ol><p><strong>Mathematical formulation</strong> for trigram &quot;ABC&quot;:</p><p class="math-container">\[\mathbf{trigram} = \mathbf{A} \odot \text{Shift}(\mathbf{B}, 1) \odot \text{Shift}(\mathbf{C}, 2)\]</p><p><strong>Example</strong>: For the word &quot;CAT&quot;:</p><ul><li>Bigrams: &quot;CA&quot;, &quot;AT&quot;</li><li><p class="math-container">\[\mathbf{CA} = \mathbf{C} \odot \text{Shift}(\mathbf{A}, 1)\]</p></li><li><p class="math-container">\[\mathbf{AT} = \mathbf{A} \odot \text{Shift}(\mathbf{T}, 1)\]</p></li><li>Word representation: <span>$\text{Bundle}(\mathbf{CA}, \mathbf{AT})$</span></li></ul><p>function encode<em>ngrams(text::String, n::Int, dimension::Int=1000)     # Create symbol dictionary     symbols = Dict{Char, Vector{Int}}()     for char in unique(text)         symbols[char] = random</em>bipolar_vector(dimension)     end</p><pre><code class="nohighlight hljs"># Generate n-grams
ngram_vectors = Vector{Vector{Int}}()

for i in 1:(length(text) - n + 1)
    ngram = text[i:i+n-1]
    ngram_vector = symbols[ngram[1]]

    for j in 2:length(ngram)
        shifted = ∏(symbols[ngram[j]], j-1)
        ngram_vector = bind(ngram_vector, shifted)
    end

    push!(ngram_vectors, ngram_vector)
end

return bundle(ngram_vectors)</code></pre><p>end</p><h1 id="Create-n-gram-encoding-for-a-sequence"><a class="docs-heading-anchor" href="#Create-n-gram-encoding-for-a-sequence">Create n-gram encoding for a sequence</a><a id="Create-n-gram-encoding-for-a-sequence-1"></a><a class="docs-heading-anchor-permalink" href="#Create-n-gram-encoding-for-a-sequence" title="Permalink"></a></h1><p>word<em>vector = encode</em>ngrams(&quot;CAT&quot;, 2) println(&quot;Encoded &#39;CAT&#39; with bigrams, vector length: &quot;, length(word_vector))</p><h3 id="3.2-Hash-Table-Encoding"><a class="docs-heading-anchor" href="#3.2-Hash-Table-Encoding">3.2 Hash Table Encoding</a><a id="3.2-Hash-Table-Encoding-1"></a><a class="docs-heading-anchor-permalink" href="#3.2-Hash-Table-Encoding" title="Permalink"></a></h3><p><strong>Hash tables</strong> in HDC map keys to values using binding, creating associative memories that can handle noise and incomplete queries.</p><p><strong>Construction</strong>:</p><ol><li>For each key-value pair <span>$(k_i, v_i)$</span>, create <span>$\mathbf{k_i} \odot \mathbf{v_i}$</span></li><li>Bundle all bound pairs: <span>$\mathbf{M} = \text{Bundle}(\mathbf{k_1} \odot \mathbf{v_1}, \mathbf{k_2} \odot \mathbf{v_2}, ...)$</span></li></ol><p><strong>Retrieval</strong>: To query key <span>$\mathbf{q}$</span>, compute <span>$\mathbf{M} \odot \mathbf{q}$</span> and find the closest stored value.</p><p><strong>Example</strong>: Storing animal-sound associations:</p><ul><li>Store: (&quot;dog&quot;, &quot;bark&quot;), (&quot;cat&quot;, &quot;meow&quot;), (&quot;cow&quot;, &quot;moo&quot;)</li><li>Memory: <span>$\mathbf{M} = \text{Bundle}(\mathbf{dog} \odot \mathbf{bark}, \mathbf{cat} \odot \mathbf{meow}, \mathbf{cow} \odot \mathbf{moo})$</span></li><li>Query &quot;dog&quot;: <span>$\mathbf{M} \odot \mathbf{dog} \approx \mathbf{bark}$</span></li></ul><p>function create<em>hash</em>table(key<em>value</em>pairs::Vector{Tuple{String, String}}, dimension::Int=1000)     # Create symbol dictionaries     keys = Dict{String, Vector{Int}}()     values = Dict{String, Vector{Int}}()</p><pre><code class="nohighlight hljs">for (k, v) in key_value_pairs
    if !haskey(keys, k)
        keys[k] = random_bipolar_vector(dimension)
    end
    if !haskey(values, v)
        values[v] = random_bipolar_vector(dimension)
    end
end

# Create bound pairs and bundle them
bound_pairs = [bind(keys[k], values[v]) for (k, v) in key_value_pairs]
memory = bundle(bound_pairs)

return (memory, keys, values)</code></pre><p>end</p><p>function query<em>hash</em>table(memory::Vector{Int}, query<em>key::Vector{Int}, values::Dict{String, Vector{Int}})     # Unbind with query key     result = bind(memory, query</em>key)</p><pre><code class="nohighlight hljs"># Find most similar value
best_match = &quot;&quot;
best_similarity = -Inf

for (value_name, value_vector) in values
    sim = cosine_similarity(result, value_vector)
    if sim &gt; best_similarity
        best_similarity = sim
        best_match = value_name
    end
end

return best_match, best_similarity</code></pre><p>end</p><h1 id="Create-and-query-HDC-hash-table"><a class="docs-heading-anchor" href="#Create-and-query-HDC-hash-table">Create and query HDC hash table</a><a id="Create-and-query-HDC-hash-table-1"></a><a class="docs-heading-anchor-permalink" href="#Create-and-query-HDC-hash-table" title="Permalink"></a></h1><p>key<em>value</em>pairs = [(&quot;dog&quot;, &quot;bark&quot;), (&quot;cat&quot;, &quot;meow&quot;), (&quot;cow&quot;, &quot;moo&quot;)] memory, keys, values = create<em>hash</em>table(key<em>value</em>pairs)</p><h1 id="Query-the-hash-table"><a class="docs-heading-anchor" href="#Query-the-hash-table">Query the hash table</a><a id="Query-the-hash-table-1"></a><a class="docs-heading-anchor-permalink" href="#Query-the-hash-table" title="Permalink"></a></h1><p>result, similarity = query<em>hash</em>table(memory, keys[&quot;dog&quot;], values) println(&quot;Query &#39;dog&#39; result: &quot;, result, &quot; (similarity: &quot;, similarity, &quot;)&quot;)</p><h2 id="4.-Basic-Learning-via-Prototype-Generation"><a class="docs-heading-anchor" href="#4.-Basic-Learning-via-Prototype-Generation">4. Basic Learning via Prototype Generation</a><a id="4.-Basic-Learning-via-Prototype-Generation-1"></a><a class="docs-heading-anchor-permalink" href="#4.-Basic-Learning-via-Prototype-Generation" title="Permalink"></a></h2><h3 id="4.1-Sum-All-Hypervectors-Approach"><a class="docs-heading-anchor" href="#4.1-Sum-All-Hypervectors-Approach">4.1 Sum-All-Hypervectors Approach</a><a id="4.1-Sum-All-Hypervectors-Approach-1"></a><a class="docs-heading-anchor-permalink" href="#4.1-Sum-All-Hypervectors-Approach" title="Permalink"></a></h3><p>The simplest learning method creates <strong>class prototypes</strong> by bundling all training examples belonging to each class.</p><p><strong>Algorithm</strong>:</p><ol><li>For each class <span>$c$</span>, bundle all training examples: <span>$\mathbf{P_c} = \text{Bundle}(\mathbf{x_1}, \mathbf{x_2}, ..., \mathbf{x_n})$</span> where <span>$\mathbf{x_i} \in c$</span></li><li>For classification, compute similarity between test example and each prototype</li><li>Assign to the most similar class</li></ol><p><strong>Language Detection Example</strong>:</p><ul><li>English prototype: Bundle all English text hypervectors</li><li>Spanish prototype: Bundle all Spanish text hypervectors</li><li>French prototype: Bundle all French text hypervectors</li></ul><p>function train<em>prototypes(training</em>data::Dict{String, Vector{Vector{Int}}})     prototypes = Dict{String, Vector{Int}}()</p><pre><code class="nohighlight hljs">for (language, examples) in training_data
    prototypes[language] = bundle(examples)
end

return prototypes</code></pre><p>end</p><p>function classify<em>text(test</em>vector::Vector{Int}, prototypes::Dict{String, Vector{Int}})     best<em>language = &quot;&quot;     best</em>similarity = -Inf</p><pre><code class="nohighlight hljs">for (language, prototype) in prototypes
    sim = cosine_similarity(test_vector, prototype)
    if sim &gt; best_similarity
        best_similarity = sim
        best_language = language
    end
end

return best_language, best_similarity</code></pre><p>end</p><h1 id="Example-with-simulated-language-data"><a class="docs-heading-anchor" href="#Example-with-simulated-language-data">Example with simulated language data</a><a id="Example-with-simulated-language-data-1"></a><a class="docs-heading-anchor-permalink" href="#Example-with-simulated-language-data" title="Permalink"></a></h1><p>english<em>examples = [random</em>bipolar<em>vector(1000) for _ in 1:10] spanish</em>examples = [random<em>bipolar</em>vector(1000) for _ in 1:10] french<em>examples = [random</em>bipolar_vector(1000) for _ in 1:10]</p><p>training<em>data = Dict(     &quot;English&quot; =&gt; english</em>examples,     &quot;Spanish&quot; =&gt; spanish<em>examples,     &quot;French&quot; =&gt; french</em>examples )</p><h1 id="Train-prototypes-for-each-language"><a class="docs-heading-anchor" href="#Train-prototypes-for-each-language">Train prototypes for each language</a><a id="Train-prototypes-for-each-language-1"></a><a class="docs-heading-anchor-permalink" href="#Train-prototypes-for-each-language" title="Permalink"></a></h1><p>prototypes = train<em>prototypes(training</em>data)</p><h1 id="Classify-new-text"><a class="docs-heading-anchor" href="#Classify-new-text">Classify new text</a><a id="Classify-new-text-1"></a><a class="docs-heading-anchor-permalink" href="#Classify-new-text" title="Permalink"></a></h1><p>test<em>text = random</em>bipolar<em>vector(1000) predicted</em>language, similarity = classify<em>text(test</em>text, prototypes) println(&quot;Predicted language: &quot;, predicted_language, &quot; (similarity: &quot;, similarity, &quot;)&quot;)</p><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>Hyperdimensional Computing provides a robust framework for representing and processing information using high-dimensional bipolar vectors. The three fundamental operations (bundling, binding, and shifting) enable complex computations while maintaining interpretability and fault tolerance. Through encodings like n-grams and hash tables, HDC can handle structured data, while learning algorithms enable adaptive classification systems that improve with experience.</p><p>The power of HDC lies in its simplicity: using only basic operations on bipolar vectors, it can solve complex problems in machine learning, natural language processing, and cognitive computing while remaining transparent and interpretable.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« HyperdimensionalComputing.jl</a><a class="docs-footer-nextpage" href="../../api/">API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Monday 28 July 2025 09:26">Monday 28 July 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
